{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Experiments Analysis\n",
    "\n",
    "This notebook combines and visualizes results from the training experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Combine All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path('../scripts/training_experiments/results')\n",
    "csv_files = list(results_dir.glob('*_metrics.csv'))\n",
    "\n",
    "# Load all CSVs\n",
    "dfs = []\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # Extract experiment name from filename\n",
    "    experiment_name = csv_file.stem.replace('_metrics', '')\n",
    "    df['experiment'] = experiment_name\n",
    "    dfs.append(df)\n",
    "\n",
    "# Combine all dataframes\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(f\"Loaded {len(csv_files)} experiment files\")\n",
    "print(f\"Total rows: {len(combined_df)}\")\n",
    "print(f\"\\nExperiments: {combined_df['experiment'].unique().tolist()}\")\n",
    "\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best epoch for each experiment based on validation loss\n",
    "best_epochs = combined_df.loc[combined_df.groupby('experiment')['val_loss'].idxmin()]\n",
    "\n",
    "# Select key metrics\n",
    "metrics_cols = ['experiment', 'epoch', 'val_loss', 'precision@1', 'MRR', 'NDCG@10',\n",
    "                'Recall@1', 'Recall@5', 'Recall@10']\n",
    "summary = best_epochs[metrics_cols].sort_values('val_loss')\n",
    "\n",
    "print(\"Best Performance (by validation loss):\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Loss Over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Embedder experiments\n",
    "embedder_df = combined_df[combined_df['model_type'] == 'embedder']\n",
    "for exp in embedder_df['experiment'].unique():\n",
    "    data = embedder_df[embedder_df['experiment'] == exp]\n",
    "    axes[0].plot(data['epoch'], data['val_loss'], marker='o', label=exp)\n",
    "axes[0].set_title('Embedder: Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Ranker experiments\n",
    "ranker_df = combined_df[combined_df['model_type'] == 'ranker']\n",
    "for exp in ranker_df['experiment'].unique():\n",
    "    data = ranker_df[ranker_df['experiment'] == exp]\n",
    "    axes[1].plot(data['epoch'], data['val_loss'], marker='o', label=exp)\n",
    "axes[1].set_title('Ranker: Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Validation Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "recall_metrics = ['Recall@1', 'Recall@5', 'Recall@10', 'Recall@20']\n",
    "\n",
    "for idx, metric in enumerate(recall_metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "\n",
    "    # Embedder\n",
    "    for exp in embedder_df['experiment'].unique():\n",
    "        data = embedder_df[embedder_df['experiment'] == exp]\n",
    "        ax.plot(data['epoch'], data[metric], marker='o', linestyle='-', label=f'{exp}')\n",
    "\n",
    "    # Ranker\n",
    "    for exp in ranker_df['experiment'].unique():\n",
    "        data = ranker_df[ranker_df['experiment'] == exp]\n",
    "        ax.plot(data['epoch'], data[metric], marker='s', linestyle='--', label=f'{exp}')\n",
    "\n",
    "    ax.set_title(metric, fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRR and NDCG@10 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# MRR\n",
    "for exp in embedder_df['experiment'].unique():\n",
    "    data = embedder_df[embedder_df['experiment'] == exp]\n",
    "    axes[0].plot(data['epoch'], data['MRR'], marker='o', linestyle='-', label=exp)\n",
    "for exp in ranker_df['experiment'].unique():\n",
    "    data = ranker_df[ranker_df['experiment'] == exp]\n",
    "    axes[0].plot(data['epoch'], data['MRR'], marker='s', linestyle='--', label=exp)\n",
    "axes[0].set_title('Mean Reciprocal Rank (MRR)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MRR')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# NDCG@10\n",
    "for exp in embedder_df['experiment'].unique():\n",
    "    data = embedder_df[embedder_df['experiment'] == exp]\n",
    "    axes[1].plot(data['epoch'], data['NDCG@10'], marker='o', linestyle='-', label=exp)\n",
    "for exp in ranker_df['experiment'].unique():\n",
    "    data = ranker_df[ranker_df['experiment'] == exp]\n",
    "    axes[1].plot(data['epoch'], data['NDCG@10'], marker='s', linestyle='--', label=exp)\n",
    "axes[1].set_title('Normalized Discounted Cumulative Gain @10', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('NDCG@10')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Performance Comparison (Bar Chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison of best epochs\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics_to_compare = ['MRR', 'NDCG@10', 'Recall@5', 'Recall@10']\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_compare):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "\n",
    "    data = best_epochs.sort_values(metric, ascending=False)\n",
    "\n",
    "    # Color by model type\n",
    "    colors = ['#1f77b4' if 'embedder' in exp else '#ff7f0e' for exp in data['experiment']]\n",
    "\n",
    "    ax.barh(data['experiment'], data[metric], color=colors)\n",
    "    ax.set_xlabel(metric, fontweight='bold')\n",
    "    ax.set_title(f'Best {metric} by Experiment', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='#1f77b4', label='Embedder'),\n",
    "                   Patch(facecolor='#ff7f0e', label='Ranker')]\n",
    "fig.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(0.98, 0.98))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the effect of freezing encoder vs query\n",
    "config_summary = best_epochs[['experiment', 'model_type', 'freeze_encoder', 'freeze_query',\n",
    "                               'MRR', 'NDCG@10', 'Recall@10', 'val_loss']].copy()\n",
    "\n",
    "# Create configuration label\n",
    "def config_label(row):\n",
    "    if row['freeze_encoder'] and row['freeze_query']:\n",
    "        return 'baseline (both frozen)'\n",
    "    elif not row['freeze_encoder'] and row['freeze_query']:\n",
    "        return 'encoder_only'\n",
    "    elif row['freeze_encoder'] and not row['freeze_query']:\n",
    "        return 'query_only'\n",
    "    else:\n",
    "        return 'both'\n",
    "\n",
    "config_summary['config'] = config_summary.apply(config_label, axis=1)\n",
    "\n",
    "print(\"\\nPerformance by Configuration:\")\n",
    "config_summary[['model_type', 'config', 'MRR', 'NDCG@10', 'Recall@10', 'val_loss']].sort_values(['model_type', 'MRR'], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Combined Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined results\n",
    "output_path = results_dir / 'combined_results.csv'\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "print(f\"Combined results saved to: {output_path}\")\n",
    "\n",
    "# Save best epochs summary\n",
    "summary_path = results_dir / 'best_performance_summary.csv'\n",
    "best_epochs.to_csv(summary_path, index=False)\n",
    "print(f\"Best performance summary saved to: {summary_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
